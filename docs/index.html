<!doctype html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>mesh-llm â€” Decentralised LLM Inference</title>
    <meta name="description" content="Pool GPU capacity to run LLMs at larger scale. Split inference across machines over QUIC. Open source.">
    <style>
        :root { --bg:#0a0a0a; --bg2:#0d0d0d; --border:#1a1a1a; --text:#d0d0d0; --text2:#888; --text3:#555; --green:#4c4; --green-dim:#2a5a2a; --green-bg:#0a1a0a; --blue:#6cf; --blue-dim:#1a3a5c; --orange:#c84; --orange-dim:#5a4a2a; }
        * { box-sizing:border-box; margin:0; padding:0; }
        html { scroll-behavior:smooth; }
        body { font-family:-apple-system,BlinkMacSystemFont,"Segoe UI",sans-serif; background:var(--bg); color:var(--text); line-height:1.7; -webkit-font-smoothing:antialiased; }
        a { color:var(--blue); text-decoration:none; } a:hover { text-decoration:underline; }
        code,.mono { font-family:"SF Mono","Fira Code",monospace; }
        nav { position:fixed; top:0; left:0; right:0; z-index:100; background:rgba(10,10,10,0.85); backdrop-filter:blur(12px); border-bottom:1px solid var(--border); padding:0.7rem 2rem; display:flex; align-items:center; gap:1rem; }
        nav .logo { font-weight:700; font-size:1.05rem; color:#fff; letter-spacing:-0.02em; } nav .logo span { color:var(--green); }
        nav .links { margin-left:auto; display:flex; gap:1.5rem; font-size:0.8rem; } nav .links a { color:var(--text2); } nav .links a:hover { color:#fff; text-decoration:none; }
        section { max-width:860px; margin:0 auto; padding:4rem 2rem; }
        .hero { min-height:85vh; display:flex; flex-direction:column; align-items:center; justify-content:center; text-align:center; padding-top:5rem; }
        .hero-badge { display:inline-flex; align-items:center; gap:0.4rem; font-size:0.7rem; color:var(--green); background:var(--green-bg); border:1px solid var(--green-dim); border-radius:20px; padding:0.25rem 0.8rem; margin-bottom:1.5rem; }
        .hero-badge .dot { width:5px; height:5px; border-radius:50%; background:var(--green); }
        .hero h1 { font-size:clamp(2.2rem,5vw,3.2rem); font-weight:700; letter-spacing:-0.03em; line-height:1.15; color:#fff; max-width:660px; margin-bottom:1rem; }
        .hero p { font-size:1.05rem; color:var(--text2); max-width:540px; margin-bottom:2rem; }
        .hero-ctas { display:flex; gap:0.8rem; flex-wrap:wrap; justify-content:center; }
        .btn { display:inline-flex; align-items:center; gap:0.4rem; padding:0.6rem 1.4rem; border-radius:8px; font-size:0.85rem; font-weight:500; cursor:pointer; border:none; transition:all 0.15s; }
        .btn-primary { background:#fff; color:#000; } .btn-primary:hover { background:#e0e0e0; text-decoration:none; }
        .btn-secondary { background:transparent; color:var(--text); border:1px solid #333; } .btn-secondary:hover { border-color:#666; text-decoration:none; }
        .mesh-demo { margin:2.5rem auto 0; max-width:580px; width:100%; }
        .mesh-demo svg { width:100%; height:auto; display:block; }
        .tags { display:flex; gap:0.5rem; flex-wrap:wrap; justify-content:center; margin-top:2rem; font-size:0.7rem; }
        .tag { padding:0.2rem 0.6rem; border-radius:4px; color:var(--text3); border:1px solid var(--border); }
        .section-tag { font-size:0.65rem; text-transform:uppercase; letter-spacing:0.1em; color:var(--green); margin-bottom:0.5rem; }
        h2 { font-size:1.7rem; font-weight:700; color:#fff; letter-spacing:-0.02em; margin-bottom:0.5rem; }
        .section-lead { color:var(--text2); font-size:0.95rem; margin-bottom:2rem; max-width:540px; }
        .steps { display:grid; grid-template-columns:repeat(auto-fit,minmax(220px,1fr)); gap:1.5rem; }
        .step { background:var(--bg2); border:1px solid var(--border); border-radius:10px; padding:1.3rem; }
        .step-num { font-size:0.65rem; font-weight:700; color:var(--green); background:var(--green-bg); border:1px solid var(--green-dim); width:22px; height:22px; border-radius:50%; display:flex; align-items:center; justify-content:center; margin-bottom:0.7rem; }
        .step h3 { font-size:0.9rem; color:#fff; margin-bottom:0.3rem; }
        .step p { font-size:0.78rem; color:var(--text2); line-height:1.6; }
        .code-block { background:var(--bg2); border:1px solid var(--border); border-radius:8px; padding:1rem 1.3rem; overflow-x:auto; font-size:0.8rem; line-height:1.8; margin:1.5rem 0; }
        .code-block .comment { color:var(--text3); } .code-block .cmd { color:var(--green); } .code-block .flag { color:var(--blue); } .code-block .val { color:var(--orange); }
        .features { display:grid; grid-template-columns:repeat(auto-fit,minmax(240px,1fr)); gap:1rem; }
        .feature { background:var(--bg2); border:1px solid var(--border); border-radius:10px; padding:1.2rem; }
        .feature h3 { font-size:0.85rem; color:#fff; margin-bottom:0.25rem; }
        .feature p { font-size:0.76rem; color:var(--text2); line-height:1.55; }
        .feature-icon { font-size:1.1rem; margin-bottom:0.5rem; }
        .cta-section { text-align:center; border-top:1px solid var(--border); padding-top:3rem; }
        .cta-section p { color:var(--text2); margin-bottom:1.5rem; }
        .cta-buttons { display:flex; gap:0.8rem; justify-content:center; flex-wrap:wrap; }
        footer { text-align:center; padding:2rem; font-size:0.7rem; color:var(--text3); border-top:1px solid var(--border); }
        @media (max-width:600px) { section { padding:3rem 1.2rem; } nav { padding:0.6rem 1rem; } .steps,.features { grid-template-columns:1fr; } }
    </style>
</head>
<body>
    <nav>
        <div class="logo">mesh-<span>llm</span></div>
        <div class="links">
            <a href="#how">How it works</a>
            <a href="#features">Features</a>
            <a href="https://github.com/michaelneale/decentralized-inference">GitHub</a>
        </div>
    </nav>

    <section class="hero">
        <div class="hero-badge"><span class="dot"></span> Open source Â· Decentralized</div>
        <h1>Pool GPUs to run larger models</h1>
        <p>Split LLM inference across machines over QUIC. Each node loads only its layers from local disk. Zero weight transfer. OpenAI-compatible API on every node.</p>
        <div class="hero-ctas">
            <a href="https://github.com/michaelneale/decentralized-inference" class="btn btn-primary">Get Started</a>
            <a href="https://github.com/michaelneale/decentralized-inference" class="btn btn-secondary">View on GitHub</a>
        </div>

        <div class="mesh-demo">
            <svg viewBox="0 0 580 260" xmlns="http://www.w3.org/2000/svg">
                <line x1="290" y1="76" x2="130" y2="170" stroke="#2a5a2a" stroke-width="1.5"/>
                <line x1="290" y1="76" x2="450" y2="170" stroke="#2a5a2a" stroke-width="1.5"/>
                <line x1="130" y1="170" x2="450" y2="170" stroke="#1a3a5c" stroke-width="1" stroke-dasharray="4,3"/>
                <circle r="2.5" fill="#4c4" opacity="0.7"><animateMotion dur="2s" repeatCount="indefinite" path="M290,76 L130,170"/></circle>
                <circle r="2.5" fill="#6cf" opacity="0.5"><animateMotion dur="2s" repeatCount="indefinite" path="M130,170 L290,76"/></circle>
                <circle r="2.5" fill="#4c4" opacity="0.7"><animateMotion dur="2.2s" repeatCount="indefinite" path="M290,76 L450,170"/></circle>
                <text x="195" y="118" text-anchor="middle" fill="#333" font-size="9" font-family="monospace">QUIC Â· RPC</text>
                <text x="385" y="118" text-anchor="middle" fill="#333" font-size="9" font-family="monospace">QUIC Â· RPC</text>
                <text x="290" y="185" text-anchor="middle" fill="#333" font-size="9" font-family="monospace">gossip</text>
                <!-- Host -->
                <rect x="220" y="8" width="140" height="68" rx="6" fill="#0a1a12" stroke="#4c4" stroke-width="2"/>
                <text x="290" y="26" text-anchor="middle" fill="#4c4" font-size="9" font-family="monospace" font-weight="600">GLM-4.7-Flash</text>
                <text x="290" y="40" text-anchor="middle" fill="#6cf" font-size="10" font-family="monospace">host (103 GB)</text>
                <text x="290" y="52" text-anchor="middle" fill="#4c4" font-size="7.5" font-family="monospace">llama-server Â· :9337</text>
                <rect x="234" y="58" width="112" height="4" rx="1.5" fill="#151515"/><rect x="234" y="58" width="73" height="4" rx="1.5" fill="#2a5a2a"/>
                <text x="234" y="72" fill="#444" font-size="7" font-family="monospace">18GB / 103GB</text>
                <!-- Worker -->
                <rect x="60" y="170" width="140" height="68" rx="6" fill="#0d1a26" stroke="#1a3a5c" stroke-width="1.5"/>
                <text x="130" y="188" text-anchor="middle" fill="#6cf" font-size="9" font-family="monospace" font-weight="600">GLM-4.7-Flash</text>
                <text x="130" y="202" text-anchor="middle" fill="#6cf" font-size="10" font-family="monospace">worker (52 GB)</text>
                <text x="130" y="214" text-anchor="middle" fill="#3a3a3a" font-size="7.5" font-family="monospace">rpc-server</text>
                <rect x="74" y="220" width="112" height="4" rx="1.5" fill="#151515"/><rect x="74" y="220" width="35" height="4" rx="1.5" fill="#1a3a5c"/>
                <text x="74" y="234" fill="#444" font-size="7" font-family="monospace">18GB / 52GB</text>
                <!-- Second model -->
                <rect x="380" y="170" width="140" height="68" rx="6" fill="#0d1a26" stroke="#1a3a5c" stroke-width="1.5"/>
                <text x="450" y="188" text-anchor="middle" fill="#c84" font-size="9" font-family="monospace" font-weight="600">Qwen2.5-3B</text>
                <text x="450" y="202" text-anchor="middle" fill="#6cf" font-size="10" font-family="monospace">solo (13 GB)</text>
                <text x="450" y="214" text-anchor="middle" fill="#4c4" font-size="7.5" font-family="monospace">llama-server Â· :9337</text>
                <rect x="394" y="220" width="112" height="4" rx="1.5" fill="#151515"/><rect x="394" y="220" width="14" height="4" rx="1.5" fill="#5a4a2a"/>
                <text x="394" y="234" fill="#444" font-size="7" font-family="monospace">2GB / 13GB</text>
            </svg>
        </div>

        <div style="margin-top:2rem; max-width:680px; width:100%;">
            <img src="mesh.png" alt="mesh-llm console" style="width:100%; border-radius:10px; border:1px solid #1a1a1a;">
        </div>

        <div class="tags">
            <span class="tag">OpenAI-compatible API</span>
            <span class="tag">Layer split across GPUs</span>
            <span class="tag">Multi-model routing</span>
            <span class="tag">Demand-aware rebalancing</span>
            <span class="tag">Nostr discovery</span>
            <span class="tag">macOS + Linux</span>
        </div>
    </section>

    <section id="how">
        <div class="section-tag">How it works</div>
        <h2>Three commands</h2>
        <p class="section-lead">No coordinator, no cloud, no API keys. Machines pool their VRAM over QUIC.</p>

        <div class="steps">
            <div class="step">
                <div class="step-num">1</div>
                <h3>Start a mesh</h3>
                <p>Pick a model. mesh-llm downloads it, starts serving, prints an invite token.</p>
            </div>
            <div class="step">
                <div class="step-num">2</div>
                <h3>Others join</h3>
                <p>Paste the token or use <code>--auto</code> to discover via Nostr. The mesh auto-assigns models and splits layers by VRAM.</p>
            </div>
            <div class="step">
                <div class="step-num">3</div>
                <h3>Use it</h3>
                <p>Every node gets <code>localhost:9337/v1</code> â€” standard OpenAI API. Works with any tool.</p>
            </div>
        </div>

        <div class="code-block">
            <span class="comment"># Start a mesh with two models</span><br>
            <span class="cmd">mesh-llm</span> <span class="flag">--model</span> <span class="val">Qwen2.5-32B</span> <span class="flag">--model</span> <span class="val">GLM-4.7-Flash</span><br><br>
            <span class="comment"># Another machine joins â€” auto-assigned to whichever model needs it</span><br>
            <span class="cmd">mesh-llm</span> <span class="flag">--join</span> <span class="val">&lt;token&gt;</span><br><br>
            <span class="comment"># Or discover public meshes and join automatically</span><br>
            <span class="cmd">mesh-llm</span> <span class="flag">--auto</span><br><br>
            <span class="comment"># Route requests by model name</span><br>
            <span class="cmd">curl</span> localhost:9337/v1/chat/completions <span class="flag">-d</span> <span class="val">'{"model":"GLM-4.7-Flash-Q4_K_M", ...}'</span>
        </div>
    </section>

    <section id="features">
        <div class="section-tag">Features</div>
        <h2>Distributed inference that actually works</h2>

        <div class="features">
            <div class="feature">
                <div class="feature-icon">âš¡</div>
                <h3>Smart layer splitting</h3>
                <p>Model doesn't fit? Layers split across nodes by VRAM. Peers selected by lowest RTT first â€” 80ms hard cap keeps splits fast. Solo mode when a model fits on one machine.</p>
            </div>
            <div class="feature">
                <div class="feature-icon">ðŸ”€</div>
                <h3>Multi-model routing</h3>
                <p>Different nodes serve different models. API proxy routes by <code>model</code> field. Nodes auto-assigned based on what's needed and what's on disk.</p>
            </div>
            <div class="feature">
                <div class="feature-icon">ðŸ“Š</div>
                <h3>Demand-aware rebalancing</h3>
                <p>Request rates tracked per model, shared via gossip. Standby nodes promote to serve hot models. Dead hosts replaced within 60 seconds.</p>
            </div>
            <div class="feature">
                <div class="feature-icon">ðŸ“¡</div>
                <h3>Nostr discovery</h3>
                <p>Publish your mesh to Nostr relays. Others find it with <code>--auto</code>. Smart scoring: region match, VRAM, health probe before joining.</p>
            </div>
            <div class="feature">
                <div class="feature-icon">ðŸš€</div>
                <h3>Zero-transfer loading</h3>
                <p>Weights read from local GGUF files, not sent over the network. Model load: 111s â†’ 5s. Per-token RPC round-trips: 558 â†’ 8.</p>
            </div>
            <div class="feature">
                <div class="feature-icon">ðŸ“ˆ</div>
                <h3>Scales passively</h3>
                <p>GPU nodes gossip. Clients use lightweight routing tables â€” zero per-client server state. Event-driven: cost proportional to topology changes, not node count.</p>
            </div>
            <div class="feature">
                <div class="feature-icon">ðŸŽ¯</div>
                <h3>Speculative decoding</h3>
                <p>Draft model runs locally, proposes tokens verified in one batched pass. +38% throughput on code. Auto-detected from catalog.</p>
            </div>
            <div class="feature">
                <div class="feature-icon">ðŸ’»</div>
                <h3>Web console</h3>
                <p>Live topology, VRAM bars, model picker, built-in chat. API-driven â€” everything the console shows comes from JSON endpoints.</p>
            </div>
        </div>
    </section>

    <section class="cta-section">
        <h2>Try it</h2>
        <p>One binary. macOS Apple Silicon and Linux. MIT licensed.</p>
        <div class="cta-buttons">
            <a href="https://github.com/michaelneale/decentralized-inference#quick-start-macos-apple-silicon" class="btn btn-primary">Get Started</a>
            <a href="https://github.com/michaelneale/decentralized-inference" class="btn btn-secondary">GitHub â†’</a>
        </div>
    </section>

    <footer><p>mesh-llm â€” open source under MIT. Built with Rust, iroh, llama.cpp.</p></footer>
</body>
</html>
